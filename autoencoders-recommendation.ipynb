{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8282489,"sourceType":"datasetVersion","datasetId":4919004}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-01T18:52:10.166196Z","iopub.execute_input":"2024-05-01T18:52:10.167370Z","iopub.status.idle":"2024-05-01T18:52:10.650234Z","shell.execute_reply.started":"2024-05-01T18:52:10.167315Z","shell.execute_reply":"2024-05-01T18:52:10.648933Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/autoencoders/AutoEncoders/autoencoders.py\n/kaggle/input/autoencoders/AutoEncoders/AutoEncoders.ipynb\n/kaggle/input/autoencoders/AutoEncoders/ml-1m/ml-1m/test_set.csv\n/kaggle/input/autoencoders/AutoEncoders/ml-1m/ml-1m/users.dat\n/kaggle/input/autoencoders/AutoEncoders/ml-1m/ml-1m/ratings.dat\n/kaggle/input/autoencoders/AutoEncoders/ml-1m/ml-1m/README\n/kaggle/input/autoencoders/AutoEncoders/ml-1m/ml-1m/.Rhistory\n/kaggle/input/autoencoders/AutoEncoders/ml-1m/ml-1m/training_set.csv\n/kaggle/input/autoencoders/AutoEncoders/ml-1m/ml-1m/movies.dat\n/kaggle/input/autoencoders/AutoEncoders/ml-100k/ml-100k/u.occupation\n/kaggle/input/autoencoders/AutoEncoders/ml-100k/ml-100k/u1.base\n/kaggle/input/autoencoders/AutoEncoders/ml-100k/ml-100k/u.info\n/kaggle/input/autoencoders/AutoEncoders/ml-100k/ml-100k/u4.test\n/kaggle/input/autoencoders/AutoEncoders/ml-100k/ml-100k/u.item\n/kaggle/input/autoencoders/AutoEncoders/ml-100k/ml-100k/README\n/kaggle/input/autoencoders/AutoEncoders/ml-100k/ml-100k/u1.test\n/kaggle/input/autoencoders/AutoEncoders/ml-100k/ml-100k/ua.test\n/kaggle/input/autoencoders/AutoEncoders/ml-100k/ml-100k/u.data\n/kaggle/input/autoencoders/AutoEncoders/ml-100k/ml-100k/u5.test\n/kaggle/input/autoencoders/AutoEncoders/ml-100k/ml-100k/mku.sh\n/kaggle/input/autoencoders/AutoEncoders/ml-100k/ml-100k/u5.base\n/kaggle/input/autoencoders/AutoEncoders/ml-100k/ml-100k/u.user\n/kaggle/input/autoencoders/AutoEncoders/ml-100k/ml-100k/ub.base\n/kaggle/input/autoencoders/AutoEncoders/ml-100k/ml-100k/u4.base\n/kaggle/input/autoencoders/AutoEncoders/ml-100k/ml-100k/u2.test\n/kaggle/input/autoencoders/AutoEncoders/ml-100k/ml-100k/ua.base\n/kaggle/input/autoencoders/AutoEncoders/ml-100k/ml-100k/u3.test\n/kaggle/input/autoencoders/AutoEncoders/ml-100k/ml-100k/u.genre\n/kaggle/input/autoencoders/AutoEncoders/ml-100k/ml-100k/allbut.pl\n/kaggle/input/autoencoders/AutoEncoders/ml-100k/ml-100k/u3.base\n/kaggle/input/autoencoders/AutoEncoders/ml-100k/ml-100k/u2.base\n/kaggle/input/autoencoders/AutoEncoders/ml-100k/ml-100k/ub.test\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.optim as optim\nimport torch.utils.data\nfrom torch.autograd import Variable\n","metadata":{"execution":{"iopub.status.busy":"2024-05-01T18:52:10.652056Z","iopub.execute_input":"2024-05-01T18:52:10.652591Z","iopub.status.idle":"2024-05-01T18:52:12.568316Z","shell.execute_reply.started":"2024-05-01T18:52:10.652561Z","shell.execute_reply":"2024-05-01T18:52:12.567397Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"movies= pd.read_csv('/kaggle/input/autoencoders/AutoEncoders/ml-1m/ml-1m/movies.dat',\n                    sep='::',\n                    header=None,\n                    engine='python',\n                   encoding='latin-1')\nusers= pd.read_csv('/kaggle/input/autoencoders/AutoEncoders/ml-1m/ml-1m/users.dat',\n                    sep='::',\n                    header=None,\n                    engine='python',\n                   encoding='latin-1')","metadata":{"execution":{"iopub.status.busy":"2024-05-01T18:52:12.569502Z","iopub.execute_input":"2024-05-01T18:52:12.570668Z","iopub.status.idle":"2024-05-01T18:52:12.656024Z","shell.execute_reply.started":"2024-05-01T18:52:12.570637Z","shell.execute_reply":"2024-05-01T18:52:12.655002Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"movies.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-01T18:52:12.658622Z","iopub.execute_input":"2024-05-01T18:52:12.658934Z","iopub.status.idle":"2024-05-01T18:52:12.679889Z","shell.execute_reply.started":"2024-05-01T18:52:12.658908Z","shell.execute_reply":"2024-05-01T18:52:12.678732Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   0                                   1                             2\n0  1                    Toy Story (1995)   Animation|Children's|Comedy\n1  2                      Jumanji (1995)  Adventure|Children's|Fantasy\n2  3             Grumpier Old Men (1995)                Comedy|Romance\n3  4            Waiting to Exhale (1995)                  Comedy|Drama\n4  5  Father of the Bride Part II (1995)                        Comedy","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Toy Story (1995)</td>\n      <td>Animation|Children's|Comedy</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Jumanji (1995)</td>\n      <td>Adventure|Children's|Fantasy</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Grumpier Old Men (1995)</td>\n      <td>Comedy|Romance</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Waiting to Exhale (1995)</td>\n      <td>Comedy|Drama</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>Father of the Bride Part II (1995)</td>\n      <td>Comedy</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"users.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-01T18:52:12.682195Z","iopub.execute_input":"2024-05-01T18:52:12.682626Z","iopub.status.idle":"2024-05-01T18:52:12.694229Z","shell.execute_reply.started":"2024-05-01T18:52:12.682588Z","shell.execute_reply":"2024-05-01T18:52:12.692818Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   0  1   2   3      4\n0  1  F   1  10  48067\n1  2  M  56  16  70072\n2  3  M  25  15  55117\n3  4  M  45   7  02460\n4  5  M  25  20  55455","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>F</td>\n      <td>1</td>\n      <td>10</td>\n      <td>48067</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>M</td>\n      <td>56</td>\n      <td>16</td>\n      <td>70072</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>M</td>\n      <td>25</td>\n      <td>15</td>\n      <td>55117</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>M</td>\n      <td>45</td>\n      <td>7</td>\n      <td>02460</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>M</td>\n      <td>25</td>\n      <td>20</td>\n      <td>55455</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"ratings=pd.read_csv('/kaggle/input/autoencoders/AutoEncoders/ml-1m/ml-1m/ratings.dat',\n                    sep='::',\n                    header=None,\n                    engine='python',\n                   encoding='latin-1')","metadata":{"execution":{"iopub.status.busy":"2024-05-01T18:52:12.696257Z","iopub.execute_input":"2024-05-01T18:52:12.696720Z","iopub.status.idle":"2024-05-01T18:52:21.503148Z","shell.execute_reply.started":"2024-05-01T18:52:12.696683Z","shell.execute_reply":"2024-05-01T18:52:21.501939Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"ratings.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-01T18:52:21.504765Z","iopub.execute_input":"2024-05-01T18:52:21.505221Z","iopub.status.idle":"2024-05-01T18:52:21.516151Z","shell.execute_reply.started":"2024-05-01T18:52:21.505170Z","shell.execute_reply":"2024-05-01T18:52:21.514995Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"   0     1  2          3\n0  1  1193  5  978300760\n1  1   661  3  978302109\n2  1   914  3  978301968\n3  1  3408  4  978300275\n4  1  2355  5  978824291","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1193</td>\n      <td>5</td>\n      <td>978300760</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>661</td>\n      <td>3</td>\n      <td>978302109</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>914</td>\n      <td>3</td>\n      <td>978301968</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>3408</td>\n      <td>4</td>\n      <td>978300275</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>2355</td>\n      <td>5</td>\n      <td>978824291</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#Preparing the train-test split\ntraining_set= pd.read_csv('/kaggle/input/autoencoders/AutoEncoders/ml-100k/ml-100k/u1.base',delimiter='\\t')\ntest_set= pd.read_csv('/kaggle/input/autoencoders/AutoEncoders/ml-100k/ml-100k/u1.test',delimiter='\\t')","metadata":{"execution":{"iopub.status.busy":"2024-05-01T18:52:21.517740Z","iopub.execute_input":"2024-05-01T18:52:21.518204Z","iopub.status.idle":"2024-05-01T18:52:21.591608Z","shell.execute_reply.started":"2024-05-01T18:52:21.518176Z","shell.execute_reply":"2024-05-01T18:52:21.590480Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"training_set=np.array(training_set,dtype='int')\ntest_set=np.array(test_set,dtype='int')","metadata":{"execution":{"iopub.status.busy":"2024-05-01T18:52:21.593362Z","iopub.execute_input":"2024-05-01T18:52:21.593693Z","iopub.status.idle":"2024-05-01T18:52:21.599535Z","shell.execute_reply.started":"2024-05-01T18:52:21.593666Z","shell.execute_reply":"2024-05-01T18:52:21.598443Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"training_set[0]","metadata":{"execution":{"iopub.status.busy":"2024-05-01T18:52:21.602726Z","iopub.execute_input":"2024-05-01T18:52:21.603063Z","iopub.status.idle":"2024-05-01T18:52:21.609155Z","shell.execute_reply.started":"2024-05-01T18:52:21.603021Z","shell.execute_reply":"2024-05-01T18:52:21.608106Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"array([        1,         2,         3, 876893171])"},"metadata":{}}]},{"cell_type":"code","source":"#Getting the number of users and movies\nnb_users=int(max(max(training_set[:,0]),max(test_set[:,0])))\nnb_movies= int(max(max(training_set[:,1]),max(test_set[:,1])))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-01T18:52:21.610411Z","iopub.execute_input":"2024-05-01T18:52:21.610919Z","iopub.status.idle":"2024-05-01T18:52:21.650095Z","shell.execute_reply.started":"2024-05-01T18:52:21.610892Z","shell.execute_reply":"2024-05-01T18:52:21.649188Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#Converting the data into an array with users in lines and movies in columns:\ndef convert(data):\n    new_data=[]\n    for id_users in range(1,nb_users+1):\n        id_movies=data[:,1][data[:,0]==id_users]\n        id_ratings=data[:,2][data[:,0]==id_users]\n        ratings=np.zeros(nb_movies)\n        ratings[id_movies-1]=id_ratings\n        new_data.append(list(ratings))\n    return new_data\ntraining_set=convert(training_set)\ntest_set=convert(test_set)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T18:52:21.651460Z","iopub.execute_input":"2024-05-01T18:52:21.651754Z","iopub.status.idle":"2024-05-01T18:52:22.135903Z","shell.execute_reply.started":"2024-05-01T18:52:21.651729Z","shell.execute_reply":"2024-05-01T18:52:22.135097Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"len(training_set),len(training_set[0])","metadata":{"execution":{"iopub.status.busy":"2024-05-01T18:52:22.136970Z","iopub.execute_input":"2024-05-01T18:52:22.137885Z","iopub.status.idle":"2024-05-01T18:52:22.144535Z","shell.execute_reply.started":"2024-05-01T18:52:22.137854Z","shell.execute_reply":"2024-05-01T18:52:22.143315Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"(943, 1682)"},"metadata":{}}]},{"cell_type":"code","source":"len(test_set),len(test_set[0])","metadata":{"execution":{"iopub.status.busy":"2024-05-01T18:52:22.145723Z","iopub.execute_input":"2024-05-01T18:52:22.146102Z","iopub.status.idle":"2024-05-01T18:52:22.154917Z","shell.execute_reply.started":"2024-05-01T18:52:22.146063Z","shell.execute_reply":"2024-05-01T18:52:22.153916Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"(943, 1682)"},"metadata":{}}]},{"cell_type":"code","source":"#Converting the data into Torch tensors\ntraining_set= torch.FloatTensor(training_set)\ntest_set= torch.FloatTensor(test_set)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T18:52:22.156133Z","iopub.execute_input":"2024-05-01T18:52:22.156567Z","iopub.status.idle":"2024-05-01T18:52:22.776441Z","shell.execute_reply.started":"2024-05-01T18:52:22.156540Z","shell.execute_reply":"2024-05-01T18:52:22.775490Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class SAE(nn.Module):\n    def __init__(self,):\n        super(SAE, self).__init__()\n        self.fc1=nn.Linear(nb_movies, 20) #random number to represent features of movies.\n        self.fc2=nn.Linear(20,10) # 20 neurons of previous layer, 10 neurons in the hidden layer.\n        self.fc3=nn.Linear(10,20) #10 neurons as input from previous layer, 20 neurons as output to next layer,because decoding now.\n        self.fc4= nn.Linear(20,nb_movies)\n        self.activation= nn.Sigmoid() # as we use Sigmoid or Hyperbolic tangent function in an autoencoder\n    def forward(self, x):\n        x=self.activation(self.fc1(x)) #This returns the encoded input vector to the second hidden layer from the original input vector.\n        x=self.activation(self.fc2(x))#This returns x into a shorter vector of 10  elements in the second hidden layer\n        x=self.activation(self.fc3(x))# Now we are decoding from input vector of 10 elements to 20 elements as output.\n        x=self.fc4(x) #Converts the previous layer 20 sized input vector into nb_movies sized output vector directly without any activation function applied.\n        return x #Vector of our predicted ratings.\n\nsae = SAE()\ncriterion = nn.MSELoss()\noptimizer = optim.RMSprop(sae.parameters(), lr= 0.01, weight_decay=0.5)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T19:58:27.045030Z","iopub.execute_input":"2024-05-01T19:58:27.045578Z","iopub.status.idle":"2024-05-01T19:58:27.060599Z","shell.execute_reply.started":"2024-05-01T19:58:27.045547Z","shell.execute_reply":"2024-05-01T19:58:27.059525Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"So, we use this super function to be able to use the methods and classes from the nn module and the one we need and we are gonna use is the Linear class, which will make the different full connections between the layers\n\nfc1  is the object of nn module that represents the full connection between this first input vector features and the first encoded vector.\n\nIn function forward:\nWe did first two encodings of our first 2 vectors features and then 2 decodings to get our reconstructed output vector in our output layer.","metadata":{}},{"cell_type":"code","source":"nb_epoch = 200\nfor epoch in range(1, nb_epoch + 1):\n  train_loss = 0\n  s = 0.\n  for id_user in range(nb_users):\n    input = Variable(training_set[id_user]).unsqueeze(0)\n    target = input.clone()\n    if torch.sum(target.data > 0) > 0:\n      output = sae(input)\n      target.require_grad = False\n      output[target == 0] = 0\n      loss = criterion(output, target)\n      mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n      loss.backward()\n      train_loss += np.sqrt(loss.data*mean_corrector)\n      s += 1.\n      optimizer.step()\n  print('epoch: '+str(epoch)+'loss: '+ str(train_loss/s))","metadata":{"execution":{"iopub.status.busy":"2024-05-01T20:36:18.156942Z","iopub.execute_input":"2024-05-01T20:36:18.157345Z","iopub.status.idle":"2024-05-01T20:41:27.159165Z","shell.execute_reply.started":"2024-05-01T20:36:18.157307Z","shell.execute_reply":"2024-05-01T20:41:27.158350Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"epoch: 1loss: tensor(1.7658)\nepoch: 2loss: tensor(1.0966)\nepoch: 3loss: tensor(1.0534)\nepoch: 4loss: tensor(1.0384)\nepoch: 5loss: tensor(1.0307)\nepoch: 6loss: tensor(1.0266)\nepoch: 7loss: tensor(1.0237)\nepoch: 8loss: tensor(1.0217)\nepoch: 9loss: tensor(1.0206)\nepoch: 10loss: tensor(1.0196)\nepoch: 11loss: tensor(1.0190)\nepoch: 12loss: tensor(1.0182)\nepoch: 13loss: tensor(1.0179)\nepoch: 14loss: tensor(1.0177)\nepoch: 15loss: tensor(1.0170)\nepoch: 16loss: tensor(1.0168)\nepoch: 17loss: tensor(1.0168)\nepoch: 18loss: tensor(1.0164)\nepoch: 19loss: tensor(1.0162)\nepoch: 20loss: tensor(1.0163)\nepoch: 21loss: tensor(1.0157)\nepoch: 22loss: tensor(1.0160)\nepoch: 23loss: tensor(1.0157)\nepoch: 24loss: tensor(1.0158)\nepoch: 25loss: tensor(1.0155)\nepoch: 26loss: tensor(1.0156)\nepoch: 27loss: tensor(1.0154)\nepoch: 28loss: tensor(1.0151)\nepoch: 29loss: tensor(1.0127)\nepoch: 30loss: tensor(1.0113)\nepoch: 31loss: tensor(1.0098)\nepoch: 32loss: tensor(1.0071)\nepoch: 33loss: tensor(1.0076)\nepoch: 34loss: tensor(1.0039)\nepoch: 35loss: tensor(1.0014)\nepoch: 36loss: tensor(0.9978)\nepoch: 37loss: tensor(0.9960)\nepoch: 38loss: tensor(0.9950)\nepoch: 39loss: tensor(0.9938)\nepoch: 40loss: tensor(0.9905)\nepoch: 41loss: tensor(0.9913)\nepoch: 42loss: tensor(0.9906)\nepoch: 43loss: tensor(0.9909)\nepoch: 44loss: tensor(0.9862)\nepoch: 45loss: tensor(0.9876)\nepoch: 46loss: tensor(0.9812)\nepoch: 47loss: tensor(0.9830)\nepoch: 48loss: tensor(0.9805)\nepoch: 49loss: tensor(0.9855)\nepoch: 50loss: tensor(0.9793)\nepoch: 51loss: tensor(0.9792)\nepoch: 52loss: tensor(0.9746)\nepoch: 53loss: tensor(0.9837)\nepoch: 54loss: tensor(0.9766)\nepoch: 55loss: tensor(0.9773)\nepoch: 56loss: tensor(0.9739)\nepoch: 57loss: tensor(0.9746)\nepoch: 58loss: tensor(0.9742)\nepoch: 59loss: tensor(0.9719)\nepoch: 60loss: tensor(0.9716)\nepoch: 61loss: tensor(0.9734)\nepoch: 62loss: tensor(0.9693)\nepoch: 63loss: tensor(0.9730)\nepoch: 64loss: tensor(0.9687)\nepoch: 65loss: tensor(0.9647)\nepoch: 66loss: tensor(0.9654)\nepoch: 67loss: tensor(0.9625)\nepoch: 68loss: tensor(0.9628)\nepoch: 69loss: tensor(0.9643)\nepoch: 70loss: tensor(0.9590)\nepoch: 71loss: tensor(0.9579)\nepoch: 72loss: tensor(0.9557)\nepoch: 73loss: tensor(0.9587)\nepoch: 74loss: tensor(0.9559)\nepoch: 75loss: tensor(0.9553)\nepoch: 76loss: tensor(0.9508)\nepoch: 77loss: tensor(0.9516)\nepoch: 78loss: tensor(0.9487)\nepoch: 79loss: tensor(0.9496)\nepoch: 80loss: tensor(0.9467)\nepoch: 81loss: tensor(0.9465)\nepoch: 82loss: tensor(0.9450)\nepoch: 83loss: tensor(0.9447)\nepoch: 84loss: tensor(0.9440)\nepoch: 85loss: tensor(0.9471)\nepoch: 86loss: tensor(0.9429)\nepoch: 87loss: tensor(0.9435)\nepoch: 88loss: tensor(0.9410)\nepoch: 89loss: tensor(0.9415)\nepoch: 90loss: tensor(0.9402)\nepoch: 91loss: tensor(0.9402)\nepoch: 92loss: tensor(0.9393)\nepoch: 93loss: tensor(0.9386)\nepoch: 94loss: tensor(0.9387)\nepoch: 95loss: tensor(0.9375)\nepoch: 96loss: tensor(0.9377)\nepoch: 97loss: tensor(0.9363)\nepoch: 98loss: tensor(0.9360)\nepoch: 99loss: tensor(0.9350)\nepoch: 100loss: tensor(0.9355)\nepoch: 101loss: tensor(0.9345)\nepoch: 102loss: tensor(0.9347)\nepoch: 103loss: tensor(0.9335)\nepoch: 104loss: tensor(0.9331)\nepoch: 105loss: tensor(0.9333)\nepoch: 106loss: tensor(0.9324)\nepoch: 107loss: tensor(0.9320)\nepoch: 108loss: tensor(0.9320)\nepoch: 109loss: tensor(0.9315)\nepoch: 110loss: tensor(0.9312)\nepoch: 111loss: tensor(0.9308)\nepoch: 112loss: tensor(0.9304)\nepoch: 113loss: tensor(0.9303)\nepoch: 114loss: tensor(0.9299)\nepoch: 115loss: tensor(0.9299)\nepoch: 116loss: tensor(0.9293)\nepoch: 117loss: tensor(0.9291)\nepoch: 118loss: tensor(0.9287)\nepoch: 119loss: tensor(0.9285)\nepoch: 120loss: tensor(0.9282)\nepoch: 121loss: tensor(0.9280)\nepoch: 122loss: tensor(0.9274)\nepoch: 123loss: tensor(0.9272)\nepoch: 124loss: tensor(0.9265)\nepoch: 125loss: tensor(0.9262)\nepoch: 126loss: tensor(0.9261)\nepoch: 127loss: tensor(0.9257)\nepoch: 128loss: tensor(0.9255)\nepoch: 129loss: tensor(0.9254)\nepoch: 130loss: tensor(0.9248)\nepoch: 131loss: tensor(0.9243)\nepoch: 132loss: tensor(0.9244)\nepoch: 133loss: tensor(0.9234)\nepoch: 134loss: tensor(0.9232)\nepoch: 135loss: tensor(0.9229)\nepoch: 136loss: tensor(0.9223)\nepoch: 137loss: tensor(0.9225)\nepoch: 138loss: tensor(0.9222)\nepoch: 139loss: tensor(0.9218)\nepoch: 140loss: tensor(0.9219)\nepoch: 141loss: tensor(0.9216)\nepoch: 142loss: tensor(0.9213)\nepoch: 143loss: tensor(0.9211)\nepoch: 144loss: tensor(0.9209)\nepoch: 145loss: tensor(0.9207)\nepoch: 146loss: tensor(0.9206)\nepoch: 147loss: tensor(0.9201)\nepoch: 148loss: tensor(0.9203)\nepoch: 149loss: tensor(0.9192)\nepoch: 150loss: tensor(0.9193)\nepoch: 151loss: tensor(0.9192)\nepoch: 152loss: tensor(0.9194)\nepoch: 153loss: tensor(0.9190)\nepoch: 154loss: tensor(0.9194)\nepoch: 155loss: tensor(0.9190)\nepoch: 156loss: tensor(0.9189)\nepoch: 157loss: tensor(0.9191)\nepoch: 158loss: tensor(0.9186)\nepoch: 159loss: tensor(0.9180)\nepoch: 160loss: tensor(0.9179)\nepoch: 161loss: tensor(0.9173)\nepoch: 162loss: tensor(0.9177)\nepoch: 163loss: tensor(0.9174)\nepoch: 164loss: tensor(0.9169)\nepoch: 165loss: tensor(0.9170)\nepoch: 166loss: tensor(0.9173)\nepoch: 167loss: tensor(0.9165)\nepoch: 168loss: tensor(0.9170)\nepoch: 169loss: tensor(0.9163)\nepoch: 170loss: tensor(0.9166)\nepoch: 171loss: tensor(0.9162)\nepoch: 172loss: tensor(0.9161)\nepoch: 173loss: tensor(0.9155)\nepoch: 174loss: tensor(0.9156)\nepoch: 175loss: tensor(0.9153)\nepoch: 176loss: tensor(0.9174)\nepoch: 177loss: tensor(0.9183)\nepoch: 178loss: tensor(0.9149)\nepoch: 179loss: tensor(0.9150)\nepoch: 180loss: tensor(0.9149)\nepoch: 181loss: tensor(0.9166)\nepoch: 182loss: tensor(0.9167)\nepoch: 183loss: tensor(0.9144)\nepoch: 184loss: tensor(0.9144)\nepoch: 185loss: tensor(0.9145)\nepoch: 186loss: tensor(0.9144)\nepoch: 187loss: tensor(0.9143)\nepoch: 188loss: tensor(0.9147)\nepoch: 189loss: tensor(0.9140)\nepoch: 190loss: tensor(0.9139)\nepoch: 191loss: tensor(0.9135)\nepoch: 192loss: tensor(0.9139)\nepoch: 193loss: tensor(0.9131)\nepoch: 194loss: tensor(0.9135)\nepoch: 195loss: tensor(0.9127)\nepoch: 196loss: tensor(0.9129)\nepoch: 197loss: tensor(0.9125)\nepoch: 198loss: tensor(0.9131)\nepoch: 199loss: tensor(0.9128)\nepoch: 200loss: tensor(0.9126)\n","output_type":"stream"}]},{"cell_type":"markdown","source":" s = 0. .....Keeps track of users who have rated at least one movie.\n for id_user in range(nb_users): ....Takes indices from 0-942 in the training set\n if torch.sum(target.data>0)>0: ....Observations that dont contain zeros, or users who rated at least one movie\n  target.require_grad= False .....Makes sure we dont compute the gradient with respect to the target.\n   output[target==0]= 0 .....These values will not count in the computations of the error and hence wont have any impact in the error counting.\n   s+=1. .....Increment s, as the user has rated at least one movie. since he/she came inside this loop.\n \n nb_movies/float(torch.sum(target.data>0)+ 1e-10) .....With 1e-10 we are making sure that the denominator is always >0\n\n\ninput= Variable(training_set[id_user]).unsqueeze(0) \nThe 0 is the index of the new dimension. And the above statement will create a batch of a single input vector.\nThe batch can have several input vectors(batch learning).\nWe are going to update the weights after each observation going to the network. And therefore we are creating a batch\nof one input vector. But we have to create this batch, otherwise it won't work.\n\nMean corrector represents the average of the error, but by only considering the movies that were rated, the movies that got atleast 1 to 5 ratings.\n\nloss.data[0] : We access the data in the loss object and then we need to take the index of the data that contains the train loss, and the index of the train loss is 0. Hence data[0]","metadata":{}},{"cell_type":"code","source":"#Testing the SAE:\ntest_loss=0\ns=0.\nfor id_user in range(nb_users):\n    input= Variable(training_set[id_user]).unsqueeze(0) # We need the training set to predict ratings of movies the user has not watched yet.\n    target= Variable(test_set[id_user]).unsqueeze(0)\n    if torch.sum(target.data>0)>0:\n        output=  sae(input) #Stores predctions on the missing input data in the training set. Returns vector of predicted ratings.\n        target.required_grad=False\n        output[target==0]=0 #Only consider the ratings of the movies that are non zero ratings in the test set because target corresponds to the real ratings in the test set and we dont want to measure the future loss on these movies htat didn't get any ratings.\n        loss=criterion(output,target)\n        mean_corrector=nb_movies/float(torch.sum(target.data>0)+1e-10)\n        test_loss+=np.sqrt(loss.data*mean_corrector)\n        s+=1. #Only considering users that gave at least a non-zero rating.\nprint('test loss: '+str(test_loss/s))\n    ","metadata":{"execution":{"iopub.status.busy":"2024-05-01T20:57:44.344108Z","iopub.execute_input":"2024-05-01T20:57:44.344534Z","iopub.status.idle":"2024-05-01T20:57:44.589509Z","shell.execute_reply.started":"2024-05-01T20:57:44.344498Z","shell.execute_reply":"2024-05-01T20:57:44.588410Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"test loss: tensor(0.9510)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Why do we use the training set?\nWe predict ratings for the movies that each user hasnot watched yet and then make predictions for the users and the movies and then compare those ratings to the ratings given for each movie and the user which were unwatched in the training set (present) but watched in the future(test set). We compare the predicted  ratings(training) and the actual ratings(test set).\n\noutput[target==0]=0 #Only consider the ratings of the movies that are non zero ratings in the test set because target corresponds to the real ratings in the test set and we dont want to measure the future loss on these movies that didn't get any ratings.\n\ntest loss: tensor(0.9510)\nThat means if we give a movie 3 stars, this system would predict we would give the movie somewhere between 2.05 to 3.95","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}