{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8266614,"sourceType":"datasetVersion","datasetId":4907479}],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-01T08:31:29.764648Z","iopub.execute_input":"2024-05-01T08:31:29.765077Z","iopub.status.idle":"2024-05-01T08:31:30.877005Z","shell.execute_reply.started":"2024-05-01T08:31:29.765046Z","shell.execute_reply":"2024-05-01T08:31:30.875460Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/boltzmann-machines/Boltzmann_Machines/Boltzmann_Machine.ipynb\n/kaggle/input/boltzmann-machines/Boltzmann_Machines/boltzmann_machine.py\n/kaggle/input/boltzmann-machines/Boltzmann_Machines/AItRBM-proof.pdf\n/kaggle/input/boltzmann-machines/Boltzmann_Machines/ml-1m/Train_Test_Set_Creation.R\n/kaggle/input/boltzmann-machines/Boltzmann_Machines/ml-1m/test_set.csv\n/kaggle/input/boltzmann-machines/Boltzmann_Machines/ml-1m/ratings.csv\n/kaggle/input/boltzmann-machines/Boltzmann_Machines/ml-1m/users.dat\n/kaggle/input/boltzmann-machines/Boltzmann_Machines/ml-1m/ratings.dat\n/kaggle/input/boltzmann-machines/Boltzmann_Machines/ml-1m/README\n/kaggle/input/boltzmann-machines/Boltzmann_Machines/ml-1m/.Rhistory\n/kaggle/input/boltzmann-machines/Boltzmann_Machines/ml-1m/training_set.csv\n/kaggle/input/boltzmann-machines/Boltzmann_Machines/ml-1m/movies.dat\n/kaggle/input/boltzmann-machines/Boltzmann_Machines/ml-100k/u.occupation\n/kaggle/input/boltzmann-machines/Boltzmann_Machines/ml-100k/u1.base\n/kaggle/input/boltzmann-machines/Boltzmann_Machines/ml-100k/u.info\n/kaggle/input/boltzmann-machines/Boltzmann_Machines/ml-100k/u4.test\n/kaggle/input/boltzmann-machines/Boltzmann_Machines/ml-100k/u.item\n/kaggle/input/boltzmann-machines/Boltzmann_Machines/ml-100k/README\n/kaggle/input/boltzmann-machines/Boltzmann_Machines/ml-100k/u1.test\n/kaggle/input/boltzmann-machines/Boltzmann_Machines/ml-100k/ua.test\n/kaggle/input/boltzmann-machines/Boltzmann_Machines/ml-100k/u.data\n/kaggle/input/boltzmann-machines/Boltzmann_Machines/ml-100k/u5.test\n/kaggle/input/boltzmann-machines/Boltzmann_Machines/ml-100k/mku.sh\n/kaggle/input/boltzmann-machines/Boltzmann_Machines/ml-100k/u5.base\n/kaggle/input/boltzmann-machines/Boltzmann_Machines/ml-100k/u.user\n/kaggle/input/boltzmann-machines/Boltzmann_Machines/ml-100k/ub.base\n/kaggle/input/boltzmann-machines/Boltzmann_Machines/ml-100k/u4.base\n/kaggle/input/boltzmann-machines/Boltzmann_Machines/ml-100k/u2.test\n/kaggle/input/boltzmann-machines/Boltzmann_Machines/ml-100k/ua.base\n/kaggle/input/boltzmann-machines/Boltzmann_Machines/ml-100k/u3.test\n/kaggle/input/boltzmann-machines/Boltzmann_Machines/ml-100k/u.genre\n/kaggle/input/boltzmann-machines/Boltzmann_Machines/ml-100k/allbut.pl\n/kaggle/input/boltzmann-machines/Boltzmann_Machines/ml-100k/u3.base\n/kaggle/input/boltzmann-machines/Boltzmann_Machines/ml-100k/u2.base\n/kaggle/input/boltzmann-machines/Boltzmann_Machines/ml-100k/ub.test\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.optim as optim\nimport torch.utils.data\nfrom torch.autograd import Variable","metadata":{"execution":{"iopub.status.busy":"2024-05-01T08:31:30.879324Z","iopub.execute_input":"2024-05-01T08:31:30.880199Z","iopub.status.idle":"2024-05-01T08:31:34.568343Z","shell.execute_reply.started":"2024-05-01T08:31:30.880162Z","shell.execute_reply":"2024-05-01T08:31:34.567017Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#Importing the dataset\n# We are using separator as :: and not a comma as some movies might have commas in their names itself.\n#Our movie file doesn't contain headers, so instead of the default header='infer', which infers\n#the headers from the column names, we put it to 'none.'\n#engine=python for efficiency.\n#Some movies contain names which can't be decoded by the default 'UTF-8' encoder so we are using\n#'latin-1' instead.\nmovies=pd.read_csv('/kaggle/input/boltzmann-machines/Boltzmann_Machines/ml-1m/movies.dat',\n                  sep ='::',\n                  header= None,\n                  engine='python',\n                  encoding='latin-1')\nusers=pd.read_csv('/kaggle/input/boltzmann-machines/Boltzmann_Machines/ml-1m/users.dat',\n                  sep ='::',\n                  header= None,\n                  engine='python',\n                  encoding='latin-1')\nratings= pd.read_csv('/kaggle/input/boltzmann-machines/Boltzmann_Machines/ml-1m/ratings.dat',\n                  sep ='::',\n                  header= None,\n                  engine='python',\n                  encoding='latin-1')","metadata":{"execution":{"iopub.status.busy":"2024-05-01T08:31:34.570116Z","iopub.execute_input":"2024-05-01T08:31:34.570802Z","iopub.status.idle":"2024-05-01T08:31:43.916236Z","shell.execute_reply.started":"2024-05-01T08:31:34.570755Z","shell.execute_reply":"2024-05-01T08:31:43.915117Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"ratings.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-01T08:31:43.917411Z","iopub.execute_input":"2024-05-01T08:31:43.917805Z","iopub.status.idle":"2024-05-01T08:31:43.939913Z","shell.execute_reply.started":"2024-05-01T08:31:43.917763Z","shell.execute_reply":"2024-05-01T08:31:43.938403Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   0     1  2          3\n0  1  1193  5  978300760\n1  1   661  3  978302109\n2  1   914  3  978301968\n3  1  3408  4  978300275\n4  1  2355  5  978824291","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1193</td>\n      <td>5</td>\n      <td>978300760</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>661</td>\n      <td>3</td>\n      <td>978302109</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>914</td>\n      <td>3</td>\n      <td>978301968</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>3408</td>\n      <td>4</td>\n      <td>978300275</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>2355</td>\n      <td>5</td>\n      <td>978824291</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# We are just taking u1,u2 focusing on autoencoders, instead of all the 5 u1-u5 train test splits\n# which are there for cross validation.\n# The dataset has `tab` as the delimiter, so we use `tab`. By default it is `comma`\n# We use `tab` with delimiter and `::` with separator as sep.\n\ntraining_set=pd.read_csv('/kaggle/input/boltzmann-machines/Boltzmann_Machines/ml-100k/u1.base',\n                        delimiter='\\t')\ntest_set=pd.read_csv('/kaggle/input/boltzmann-machines/Boltzmann_Machines/ml-100k/u1.test',\n                    delimiter='\\t')","metadata":{"execution":{"iopub.status.busy":"2024-05-01T08:31:43.944049Z","iopub.execute_input":"2024-05-01T08:31:43.944491Z","iopub.status.idle":"2024-05-01T08:31:44.023544Z","shell.execute_reply.started":"2024-05-01T08:31:43.944459Z","shell.execute_reply":"2024-05-01T08:31:44.022467Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"training_set=np.array(training_set,dtype='int')\ntest_set=np.array(test_set,dtype='int')","metadata":{"execution":{"iopub.status.busy":"2024-05-01T08:31:44.024898Z","iopub.execute_input":"2024-05-01T08:31:44.025337Z","iopub.status.idle":"2024-05-01T08:31:44.032106Z","shell.execute_reply.started":"2024-05-01T08:31:44.025298Z","shell.execute_reply":"2024-05-01T08:31:44.030903Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# We will create two matrices mat[u][i] where u: user, i:movie, m[u][i]:rating of movie i given by user u.\n#If user u didn't rate movie i, we will put a 0 in that cell, for both the training and test sets.\n\n# We are finding the max in both training and test set to get the last user and last movie,\n# as the data might be random and maximum might be present in any of the two.\nnb_users= int(max(max(training_set[:,0]),max(test_set[:,0]))) #max. of first column in the training\n              # and test sets. The first column contains the user ids.\nnb_movies= int(max(max(training_set[:,1]),max(test_set[:,1]))) #max. of second column in the training\n              # and test sets. The second column contains the movie names.","metadata":{"execution":{"iopub.status.busy":"2024-05-01T08:31:44.033567Z","iopub.execute_input":"2024-05-01T08:31:44.033980Z","iopub.status.idle":"2024-05-01T08:31:44.075101Z","shell.execute_reply.started":"2024-05-01T08:31:44.033920Z","shell.execute_reply":"2024-05-01T08:31:44.073857Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"nb_users,nb_movies","metadata":{"execution":{"iopub.status.busy":"2024-05-01T08:31:44.076588Z","iopub.execute_input":"2024-05-01T08:31:44.077045Z","iopub.status.idle":"2024-05-01T08:31:44.093390Z","shell.execute_reply.started":"2024-05-01T08:31:44.077006Z","shell.execute_reply":"2024-05-01T08:31:44.091930Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(943, 1682)"},"metadata":{}}]},{"cell_type":"code","source":"#All movie_ids are stored in  data[:,1].\n#User ids are stored in data[:,0]\n#Ratings are stored in data[:,2]\ndef convert(data):\n    new_data=[]\n    for id_users in range(1,nb_users+1):\n        id_movies=data[:,1][data[:,0]==id_users] #all the movies watched by the ith(id_user) user and rated.\n        id_ratings=data[:,2][data[:,0]==id_users] #all the ratings given by the ith(id_user) user\n        #Still we are not getting a '0'for when the user didnot rate the movie.\n        #Read Below:\n        ratings=np.zeros(nb_movies)\n        ratings[id_movies-1]= id_ratings #ratings for each of the 1682 movies given by user=id_user(i th user in loop)\n        new_data.append(list(ratings)) #list of lists, each list containing ratings given by all the different users.\n    return new_data\ntraining_set=convert(training_set)\ntest_set=convert(test_set)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T08:31:44.095060Z","iopub.execute_input":"2024-05-01T08:31:44.095428Z","iopub.status.idle":"2024-05-01T08:31:44.577324Z","shell.execute_reply.started":"2024-05-01T08:31:44.095400Z","shell.execute_reply":"2024-05-01T08:31:44.576223Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"There are 1682 movies and 982 users, so we have to create a list of 1682 users with '0'.\nFor movies, the user rated we have to replace '0' by the user rating, for that movie.\n\nAlso, indexes in python start by zero and our movie ids start by 1. Therefore ratings[id_movies-1]","metadata":{}},{"cell_type":"code","source":"# training_set_array = np.array(training_set)\n# print(training_set_array.shape)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T08:31:44.578599Z","iopub.execute_input":"2024-05-01T08:31:44.578928Z","iopub.status.idle":"2024-05-01T08:31:44.583591Z","shell.execute_reply.started":"2024-05-01T08:31:44.578900Z","shell.execute_reply":"2024-05-01T08:31:44.582357Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#If we uncomment the above command we will get (943,1682) as output after converting the list into\n# a numpy nd-array to know its dimensions.\n\n# as we have converted the nd-array training_set into list of lists.\n\n#Therefore we can see each of the 943 users have a list of 1682 movies with their ratings from 0-5 each\n# in floating point decimals format.\n\n#We had to do it in the above format because training_set is a list of lists and not a numpy nd\n#array or pandas data frame, so we had to get it into a nd-array format.\n\n# We can also get the dimensions of the list of lists by doing the command below:\n","metadata":{"execution":{"iopub.status.busy":"2024-05-01T08:31:44.584974Z","iopub.execute_input":"2024-05-01T08:31:44.585314Z","iopub.status.idle":"2024-05-01T08:31:44.595373Z","shell.execute_reply.started":"2024-05-01T08:31:44.585285Z","shell.execute_reply":"2024-05-01T08:31:44.594214Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"len(training_set),len(training_set[0])","metadata":{"execution":{"iopub.status.busy":"2024-05-01T08:31:44.596998Z","iopub.execute_input":"2024-05-01T08:31:44.597976Z","iopub.status.idle":"2024-05-01T08:31:44.612894Z","shell.execute_reply.started":"2024-05-01T08:31:44.597902Z","shell.execute_reply":"2024-05-01T08:31:44.611651Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"(943, 1682)"},"metadata":{}}]},{"cell_type":"code","source":"#Converting the data into torch tensors\ntraining_set=  torch.FloatTensor(training_set)\ntest_set=  torch.FloatTensor(test_set)#This expects a list of lists, which it converts into a torch tensor.","metadata":{"execution":{"iopub.status.busy":"2024-05-01T08:31:44.614700Z","iopub.execute_input":"2024-05-01T08:31:44.615118Z","iopub.status.idle":"2024-05-01T08:31:45.229712Z","shell.execute_reply.started":"2024-05-01T08:31:44.615087Z","shell.execute_reply":"2024-05-01T08:31:45.228626Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#Converting the ratings into binary ratings 1 (Liked) or Not Liked(0)\n#RBM will predict in 0-1 binary format whether the user liked a movie or not, therefore we need to convert\n#these ratings as well into 0-1  format, otherwise things will be inconsistent for the RBM.\n\n# Where training_set=0, means user hasn't watched the movie and we replace it by `-1`\n\ntraining_set[training_set==0]=-1;\ntraining_set[training_set<=2]=0;\ntraining_set[training_set>=3]=1;\n\ntest_set[test_set==0]=-1;\ntest_set[test_set<=2]=0;\ntest_set[test_set>=3]=1;\n","metadata":{"execution":{"iopub.status.busy":"2024-05-01T08:31:45.234279Z","iopub.execute_input":"2024-05-01T08:31:45.235154Z","iopub.status.idle":"2024-05-01T08:31:45.317570Z","shell.execute_reply.started":"2024-05-01T08:31:45.235119Z","shell.execute_reply":"2024-05-01T08:31:45.316299Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# An RBM is a probabilistic graphical model, so we will build one.\n\n#Creating the architecture of the Neural Network:\n        \n    #Input-vector(1-user), #vk-visible nodes obtained after k-samplings\n    #ph0:vector of probabilities that at the first iteration, the hidden nodes=1, given the values\n    #of v0, phk->probabilites of hidden nodes after k-sampling, given the values of visible nodes(vk).\n    \n    \nclass RBM():\n  def __init__(self, nv, nh):\n    self.W = torch.randn(nh, nv)\n    self.a = torch.randn(1, nh)\n    self.b = torch.randn(1, nv) #h-hidden node, v-visible node.\n  def sample_h(self, x): #x-visible node\n    wx = torch.mm(x, self.W.t())\n    activation = wx + self.a.expand_as(wx)\n    p_h_given_v = torch.sigmoid(activation)\n    return p_h_given_v, torch.bernoulli(p_h_given_v)\n  def sample_v(self, y): #y represents hidden nodes.\n    wy = torch.mm(y, self.W)\n    activation = wy + self.b.expand_as(wy)\n    p_v_given_h = torch.sigmoid(activation)\n    return p_v_given_h, torch.bernoulli(p_v_given_h)\n  def train(self, v0, vk, ph0, phk):\n    self.W += (torch.mm(v0.t(), ph0) - torch.mm(vk.t(), phk)).t()\n    self.b += torch.sum((v0 - vk), 0)\n    self.a += torch.sum((ph0 - phk), 0)\nnv = len(training_set[0])\nnh = 100\nbatch_size = 100\nrbm = RBM(nv, nh)\n        ","metadata":{"execution":{"iopub.status.busy":"2024-05-01T09:34:20.930103Z","iopub.execute_input":"2024-05-01T09:34:20.930526Z","iopub.status.idle":"2024-05-01T09:34:20.945707Z","shell.execute_reply.started":"2024-05-01T09:34:20.930496Z","shell.execute_reply":"2024-05-01T09:34:20.944466Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"torch.randn(nh,nv) initializes a tensor of size (nh,nv),\naccording to a normal distribution, and besides this normal distribution has\na mean of zero and variance of 1. Hence, this initializes all the weights for the probabilities.\n(P of the visible nodes according to the hidden nodes).\n\nThe biases a(for hidden nodes) and b(for visible nodes), should have 2-dimensions because that's what pytorch accepts.\nFirst dimension corresponding to the batch(fake dimension), and second dimension corresponding to the bias.\n\nPytorch only accepts a 2-D tensor\n\nthat's why we create self.a= torch.randn(1,nh)-> 1 represents the batch and nh corresponding to the bias(normalized in 0-1).\n\ndef sample_h(self, x):\n        Calculating probability of h given v,\n        probability that the hidden neuron=1 given the values of the visible neurons,\n        that is actually our input vector of observations with all the ratings\n        \n        This probability is nothing else than the sigmoid function.\n        applied to W(vector of weights)*x(vector of visible neurons)+the bias(a),\n        because a corresponds to the bias of the hidden nodes.\n        \n        torch.mm -->multiplies two matrices\n \nWe take the transpose of self.w to make it mathematically correct.\nself.a is a 2-D matrix that contains the input vector in batches(mini batch), although the batch size is 1.\nWe want to make sure that we apply this bias to each line of the mini batch, that is to each line of \nthis dimension an to do that we add a new dimension for the batch that we are adding, and this function\nis known by expand_as\n\nThe activation function will be a probability that the hidden node will be activated according to the value of the visible node.\n\np_h_given_v= probability that the hidden node is activated given the value of the visible node.\n\n\nWe are making a Bernouille RBM because we are predicting a binary outcome whether the users like (yes/no) for a movie.\n\nAnd that is what we are returning, some Bernoulli samples of that distribution, of that probabilities\nof h given v.\n\n\nph given v is a vector of 100 elements, each of these elements corresponds to each of the 100\nhidden nodes and each of the elements is the probability that the hidden node is activated.\n\n\nreturn p_h_given_v, torch.bernoulli(p_h_given_v)\nThis will return all the probabilities of the hidden neurons, given the values of the visible nodes.\nThat is the ratings and it will return also that sampling of the hidden neurons.\n\nThe function above is the first function we need for Gibb's sampling.\n\nIn function:\ndef sample_h(self, x):\n    We return the probabilities that each of the visible nodes=1,given the values of the hidden nodes.\n    given whether the hidden nodes are activated or not.\n    \n#### This is because we are recreating the Gibbs' Sampling where we recreate the Input vectors,\nand also for the logarithmic function to minimise the gradient at the original input vector.\n\n#### We must not take the transpose in the third function. Why?\nSince W is the weight matrix of p_v_given_h and we are computing p_v_given_h in the second third function.\nWe were calculating p_h_given_v in the second function that's why we had to take the transpose of W.\n\n#### Function 4:\nWe have to minimise the energy or maximize the log-likelihood, and to do that we need to compute the gradient.\nSince the direct computation of gradients is heavy, we are going to try to approximate it.\nWe do that by Contrastive Divergence in Gibbs' Sampling.\n#### Watch lecture no.110 to better understand function:4 and read the research paper as well. ","metadata":{}},{"cell_type":"code","source":"#nv: no. of visible nodes= each movie\nnv= len(training_set[0])\nnh=100 #Random number\nbatch_size=100 #Random number\nrbm=RBM(nv,nh)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T09:34:25.877766Z","iopub.execute_input":"2024-05-01T09:34:25.878190Z","iopub.status.idle":"2024-05-01T09:34:25.885189Z","shell.execute_reply.started":"2024-05-01T09:34:25.878154Z","shell.execute_reply":"2024-05-01T09:34:25.883816Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"#Training the RBM\n# we have 9000 users with binary(0,1) classification of whether or not the user liked the movie, hence we will reach\n# convergence pretty fast in around 10 epochs.\nnb_epoch=10\nfor epoch in range(1,nb_epoch+1):\n    train_loss=0 #initial loss=0 before starting training.\n    s=0. #o=counter to normalize train_loss(divide train_loss by the counter 0.= float)\n    for id_user in range(0,nb_users-batch_size,batch_size): #taking input in batch size=100\n        vk=training_set[id_user:id_user+batch_size] #visible nodes to be changed after gibbs' sampling iterations.\n        v0=training_set[id_user:id_user+batch_size] #visible nodes at the start,i.e., the original ratings\n        ph0,_= rbm.sample_h(v0) #probabilities that hidden node equal one given the visible nodes, also taking only first output returned by the function.\n        for k in range(10):# training for 10 times a/c to gibbs' sampling.\n            _,hk=rbm.sample_h(vk) #changing hidden nodes after sampling visible nodes.\n            _,vk=rbm.sample_v(hk)#changing visible nodes after sampling hidden nodes.\n            vk[v0<0] = v0[v0<0]#making sure that where movie ratings was -1( not given by user) persists even after samplings.\n        phk,_= rbm.sample_h(vk) #sample_h applied on last iterated sample of visible nodes.\n        rbm.train(v0,vk,ph0,phk)# Now weights,biases are going to be updated towards direction of maximum likelihood and the train\n        #function does not return anything it just updates the weights and biases.\n        train_loss += torch.mean(torch.abs(v0[v0 >= 0] - vk[v0 >= 0]))#vk:prediction, v0:original, v0[v0>=0]:ratings that exist/given by user.\n        s+=1. #updating counter by 1(in float)\n    print('epoch: '+str(epoch)+' loss: '+str(train_loss/s)) #normalizing the training loss","metadata":{"execution":{"iopub.status.busy":"2024-05-01T09:37:18.311206Z","iopub.execute_input":"2024-05-01T09:37:18.311652Z","iopub.status.idle":"2024-05-01T09:37:22.512106Z","shell.execute_reply.started":"2024-05-01T09:37:18.311619Z","shell.execute_reply":"2024-05-01T09:37:22.510773Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"epoch: 1 loss: tensor(0.0708)\nepoch: 2 loss: tensor(0.0712)\nepoch: 3 loss: tensor(0.0716)\nepoch: 4 loss: tensor(0.0704)\nepoch: 5 loss: tensor(0.0711)\nepoch: 6 loss: tensor(0.0724)\nepoch: 7 loss: tensor(0.0712)\nepoch: 8 loss: tensor(0.0712)\nepoch: 9 loss: tensor(0.0711)\nepoch: 10 loss: tensor(0.0715)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### ((1-0.07)/1) % of ratings correctly predicted.","metadata":{}},{"cell_type":"code","source":"#Testing the RBM\n#No training required, hence no epochs necessary.\ntest_loss = 0\ns = 0.\nfor id_user in range(nb_users):\n    v = training_set[id_user : id_user+1] #id user+1 for each user, v  is input on which we will make prediction. READ BELOW:\n    vt = test_set[id_user : id_user+1]  #target was mentioned by vt.\n    if len(vt[vt>=0])>0:\n        _,h = rbm.sample_h(v)\n        _,v = rbm.sample_v(h)\n        test_loss += torch.mean(torch.abs(vt[vt >= 0] - v[vt >= 0]))\n        s += 1.\nprint('test_loss: '+str(test_loss/s))","metadata":{"execution":{"iopub.status.busy":"2024-05-01T10:23:03.814834Z","iopub.execute_input":"2024-05-01T10:23:03.815282Z","iopub.status.idle":"2024-05-01T10:23:04.202232Z","shell.execute_reply.started":"2024-05-01T10:23:03.815249Z","shell.execute_reply":"2024-05-01T10:23:04.201002Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"test_loss: tensor(0.0670)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"vk = training_set[id_user : id_user+1] The training set will be used to activate the hidden neurons to get the outputs.\nRight now the training set contains the ratings of the training set and it doesn't contain the answers of the test set.\nBut by using the inputs of the training set, we will activate the neurons of our RBM to predict the ratings of the movies\nthat were not rated yet, and that is the ratings of the test set.\n\nTo get our predictions of the test set ratings, do we need to apply the k-step contrastive divergence or more precisely,\ndo we need k steps of the random walk(i.e.10 steps of the random walk) or 1 step of the random walk?\n-> We need to make one step because the principle of the random walk, this is not the random walk even, because in the random walk the probabilities are the same. Here, even if its a Markov chain, the probabilities are not the same so its not a random walk rather its a blind walk.\n\nRead on: MCMCM, Markov Chain Monte Carlo's blind walk techniques.\n\nAbove, we were trained on 10 steps, so we will be much better at staying on an imaginary straight line(imagine being blindfolded).\n\nSo, our prediction will be one round trip of Gibbs' Sampling, one iteration, one step of the blind walk.","metadata":{}},{"cell_type":"markdown","source":"Hence, we can see the test loss is 0.0670\nSo for new observations, we managed to predict if the user will like the movie or not by 93% ((1-0.067)/1j)","metadata":{}}]}